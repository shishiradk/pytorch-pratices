# -*- coding: utf-8 -*-
"""03_pytorch_computer_vision_exercises.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JG1W7bXKpSbnQMAOhXMdIuctAbO2Rclk

# 03. PyTorch Computer Vision Exercises

The following is a collection of exercises based on computer vision fundamentals in PyTorch.

They're a bunch of fun.

You're going to get to write plenty of code!

## Resources

1. These exercises are based on [notebook 03 of the Learn PyTorch for Deep Learning course](https://www.learnpytorch.io/03_pytorch_computer_vision/).
2. See a live [walkthrough of the solutions (errors and all) on YouTube](https://youtu.be/_PibmqpEyhA).
  * **Note:** Going through these exercises took me just over 3 hours of solid coding, so you should expect around the same.
3. See [other solutions on the course GitHub](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/extras/solutions).
"""

# Check for GPU
!nvidia-smi

# Import torch
import torch

# Exercises require PyTorch > 1.10.0
print(torch.__version__)

# TODO: Setup device agnostic code

"""## 1. What are 3 areas in industry where computer vision is currently being used?

1. Self driving car
2. Security
3. Health Care

## 2. Search "what is overfitting in machine learning" and write down a sentence about what you find.

Overfitting is the condition where model learnins training data too well but cant answer the slightly different data.

## 3. Search "ways to prevent overfitting in machine learning", write down 3 of the things you find and a sentence about each.
> **Note:** there are lots of these, so don't worry too much about all of them, just pick 3 and start with those.

1. training data should be enough to learn the true pattern instead of meamorizing
2. complex model overfit easily so simplify , for eg: using fewer feature (only necessarey one),
3. Cross validation

## 4. Spend 20-minutes reading and clicking through the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/).

* Upload your own example image using the "upload" button on the website and see what happens in each layer of a CNN as your image passes through it.

## 5. Load the [`torchvision.datasets.MNIST()`](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST) train and test datasets.
"""

import torchvision
from torchvision import datasets
from torchvision import transforms

# Get the Mnist train dataset
train_data = datasets.MNIST(root=".",
                            train=True,
                            download=True,
                            transform=transforms.ToTensor())

# Get the MNIST test datset
test_data = datasets.MNIST(root=".",
                           train=False,
                           download=True,
                           transform=transforms.ToTensor())

train_data, test_data

len(train_data),len(test_data)

# Data is in tuple form (image, label)
img = train_data[0][0]
label = train_data[0][1]
print(f"Image:\n {img}")
print(f"label:\n {label}")

#checking shapes of our data
print(f"Image shape: {img.shape} -> [color_channels, height, width] (CHW)")
print(f"label: {label} -> no shape, due to being integer")

# Get the classes names from the datset
class_names = train_data.classes
class_names

"""## 6. Visualize at least 5 different samples of the MNIST training dataset."""

import matplotlib.pyplot as plt
for i in range(5):
  img = train_data[i][0]
  print(img.shape)
  img_squeeze = img.squeeze()
  print(img_squeeze.shape)
  label = train_data[i][1]
  plt.figure(figsize=(3,3))
  plt.imshow(img_squeeze, cmap="gray")
  plt.title(label)
  plt.axis(False)

"""## 7. Turn the MNIST train and test datasets into dataloaders using `torch.utils.data.DataLoader`, set the `batch_size=32`."""

# Create train dataloader
from torch.utils.data import DataLoader

train_dataloader = DataLoader(dataset=train_data,
                              batch_size=32,
                              shuffle=True)

test_dataloader = DataLoader(dataset=test_data,
                             batch_size=32,
                             shuffle= False)

train_dataloader, test_dataloader

for sample in next(iter(train_dataloader)):
  print(sample.shape)

len(train_dataloader), len(test_dataloader)

"""## 8. Recreate `model_2` used in notebook 03 (the same model from the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/), also known as TinyVGG) capable of fitting on the MNIST dataset."""

from torch import nn
class MNIST_model(torch.nn.Module):
  """Model capable of predicting on MNIST datset."""
  def __init__(self, input_shape: int, hidden_units: int, output_shape: int):
    super().__init__()
    self.conv_block_1 = nn.Sequential(
        nn.Conv2d(in_channels=input_shape,
                  out_channels=hidden_units,
                  kernel_size=3,
                  stride=1,
                  padding=1),
        nn.ReLU(),
        nn.Conv2d(in_channels=hidden_units,
                  out_channels=hidden_units,
                  kernel_size=3,
                  stride=1,
                  padding=1),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2)
    )
    self.conv_block_2 = nn.Sequential(
        nn.Conv2d(in_channels=hidden_units,
                  out_channels=hidden_units,
                  kernel_size=3,
                  stride=1,
                  padding=1),
        nn.ReLU(),
        nn.Conv2d(in_channels=hidden_units,
                  out_channels=hidden_units,
                  kernel_size=3,
                  stride=1,
                  padding=1),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2)
    )
    self.classifier = nn.Sequential(
        nn.Flatten(),
        nn.Linear(in_features=hidden_units*7*7,
                  out_features=output_shape)
    )

  def forward(self, x ):
    x= self.conv_block_1(x)
    x = self.conv_block_2(x)
    x = self.classifier(x)
    return x

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}")

model = MNIST_model(input_shape=1,
                    hidden_units=10,
                    output_shape=10).to(device)
model

"""## 9. Train the model you built in exercise 8. for 5 epochs on CPU and GPU and see how long it takes on each."""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from tqdm.auto import tqdm
# 
# # Train on CPU
# model_cpu = MNIST_model(input_shape=1,
#                         hidden_units=10,
#                         output_shape=10).to("cpu")
# 
# # Create a loss function and optimizer
# loss_fn = nn.CrossEntropyLoss()
# optimizer = torch.optim.SGD(model_cpu.parameters(), lr=0.1)
# 
# ### Training loop
# epochs = 5
# for epoch in tqdm(range(epochs)):
#   train_loss = 0
#   for batch, (X, y) in enumerate(train_dataloader):
#     model_cpu.train()
# 
#     # Put data on CPU
#     X, y = X.to("cpu"), y.to("cpu")
# 
#     # Forward pass
#     y_pred = model_cpu(X)
# 
#     # Loss calculation
#     loss = loss_fn(y_pred, y)
#     train_loss += loss
# 
#     # Optimizer zero grad
#     optimizer.zero_grad()
# 
#     # Loss backward
#     loss.backward()
# 
#     # Step the optimizer
#     optimizer.step()
# 
#   # Adjust train loss for number of batches
#   train_loss /= len(train_dataloader)
# 
#   ### Testing loop
#   test_loss_total = 0
# 
#   # Put model in eval mode
#   model_cpu.eval()
# 
#   # Turn on inference mode
#   with torch.inference_mode():
#     for batch, (X_test, y_test) in enumerate(test_dataloader):
#       # Make sure test data on CPU
#       X_test, y_test = X_test.to("cpu"), y_test.to("cpu")
#       test_pred = model_cpu(X_test)
#       test_loss = loss_fn(test_pred, y_test)
# 
#       test_loss_total += test_loss
# 
#     test_loss_total /= len(test_dataloader)
# 
#   # Print out what's happening
#   print(f"Epoch: {epoch} | Loss: {train_loss:.3f} | Test loss: {test_loss_total:.3f}")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from tqdm.auto import tqdm
# 
# device = "cuda" if torch.cuda.is_available() else "cpu"
# 
# # Train on GPU
# model_gpu = MNIST_model(input_shape=1,
#                         hidden_units=10,
#                         output_shape=10).to(device)
# 
# # Create a loss function and optimizer
# loss_fn = nn.CrossEntropyLoss()
# optimizer = torch.optim.SGD(model_gpu.parameters(), lr=0.1)
# 
# # Training loop
# epochs = 5
# for epoch in tqdm(range(epochs)):
#   train_loss = 0
#   model_gpu.train()
#   for batch, (X, y) in enumerate(train_dataloader):
#     # Put data on target device
#     X, y = X.to(device), y.to(device)
# 
#     # Forward pass
#     y_pred = model_gpu(X)
# 
#     # Loss calculation
#     loss = loss_fn(y_pred, y)
#     train_loss += loss
# 
#     # Optimizer zero grad
#     optimizer.zero_grad()
# 
#     # Loss backward
#     loss.backward()
# 
#     # Step the optimizer
#     optimizer.step()
# 
#   # Adjust train loss to number of batches
#   train_loss /= len(train_dataloader)
# 
#   ### Testing loop
#   test_loss_total = 0
#   # Put model in eval mode and turn on inference mode
#   model_gpu.eval()
#   with torch.inference_mode():
#     for batch, (X_test, y_test) in enumerate(test_dataloader):
#       # Make sure test data on target device
#       X_test, y_test = X_test.to(device), y_test.to(device)
# 
#       test_pred = model_gpu(X_test)
#       test_loss = loss_fn(test_pred, y_test)
# 
#       test_loss_total += test_loss
# 
#     # Adjust test loss total for number of batches
#     test_loss_total /= len(test_dataloader)
# 
#   # Print out what's happening
#   print(f"Epoch: {epoch} | Loss: {train_loss:.3f} | Test loss: {test_loss_total:.3f}")

"""## 10. Make predictions using your trained model and visualize at least 5 of them comparing the prediciton to the target label."""

# Make predictions with the trained model
plt.imshow(test_data[0][0].squeeze(), cmap="gray")

#logits -> Prediction probabilities -> Prediction labels
model_pred_logits = model_gpu(test_data[0][0].unsqueeze(dim=0).to(device)) # make sure image is right shape + on right device
model_pred_probs = torch.softmax(model_pred_logits, dim=1)
model_pred_label = torch.argmax(model_pred_probs, dim=1)
model_pred_label

num_to_plot = 5
for i in range(num_to_plot):
  # Get image and labels from the test data
  img = test_data[i][0]
  label = test_data[i][1]

  # Make prediction on image
  model_pred_logits = model_gpu(img.unsqueeze(dim=0).to(device))
  model_pred_probs = torch.softmax(model_pred_logits, dim=1)
  model_pred_label = torch.argmax(model_pred_probs, dim=1)

  # Plot the image and prediction
  plt.figure()
  plt.imshow(img.squeeze(), cmap="gray")
  plt.title(f"Truth: {label} | Pred: {model_pred_label.cpu().item()}")
  plt.axis(False);

"""## 11. Plot a confusion matrix comparing your model's predictions to the truth labels."""

# see if torchmetrics exists, if not install it
try:
  import torchmetrics,mlxtend
  print(f"mlxtend version: {mlxtend.__version__}")
  assert int(mlxtend.__version__.split(".")[1]) >= 19, "mlxtend version should be 0.19.0 or higher"
except:
  !pip install -q torchmetrics -U mlxtend # <- Note: If you're using Google Colab , this may require restarting the runtime
  import torchmetrics, mlxtend
  print(f"mlxtend version: {mlxtend.__version__}")

# Import mlxtend upgraded version
import mlxtend
print(mlxtend.__version__)
assert int(mlxtend.__version__.split(".")[1]) >= 19 # should be version 0.19.0 or higher"

# Make predictions across all test data
from tqdm.auto import tqdm
model_gpu.eval()
y_preds = []
with torch.inference_mode():
  for batch, (X,y) in tqdm(enumerate(test_dataloader)):
    # Make sure data on right device
    X, y = X.to(device), y.to(device)
    # Forward pass
    y_pred_logits = model_gpu(X)
    # Logits -> Pred probs -> Pred label
    y_pred_labels = torch.argmax(torch.softmax(y_pred_logits, dim=1), dim=1)
    # Append the labels to the preds list
    y_preds.append(y_pred_labels)
  y_preds=torch.cat(y_preds).cpu()
len(y_preds)

test_data.targets[:10], y_preds[:10]

from torchmetrics import ConfusionMatrix
from mlxtend.plotting import plot_confusion_matrix

# etup confusion mattrix
confmat = ConfusionMatrix(task="multiclass", num_classes=len(class_names))
confmat_tensor = confmat(preds= y_preds,
                         target = test_data.targets)

# Plot the confusion matrix
fix, ax = plot_confusion_matrix(
    conf_mat = confmat_tensor.numpy(),
    class_names=class_names,
    figsize=(10,7)
)

"""## 12. Create a random tensor of shape `[1, 3, 64, 64]` and pass it through a `nn.Conv2d()` layer with various hyperparameter settings (these can be any settings you choose), what do you notice if the `kernel_size` parameter goes up and down?"""

random_tensor = torch.rand([1,  3, 64, 64])
random_tensor.shape

conv_layer = nn.Conv2d(in_channels=3,
                       out_channels=64,
                       kernel_size=3,
                       stride=2,
                       padding=1)

print(f"Random tensor original shape: {random_tensor.shape}")
random_tensor_through_conv_layer = conv_layer(random_tensor)
print(f"Random tensor through conv layer shape: {random_tensor_through_conv_layer.shape}")

"""## 13. Use a model similar to the trained `model_2` from notebook 03 to make predictions on the test [`torchvision.datasets.FashionMNIST`](https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html) dataset.
* Then plot some predictions where the model was wrong alongside what the label of the image should've been.
* After visualing these predictions do you think it's more of a modelling error or a data error?
* As in, could the model do better or are the labels of the data too close to each other (e.g. a "Shirt" label is too close to "T-shirt/top")?
"""

from scipy.spatial import transform
# Download FashionMNIST train & test
from torchvision import datasets
from torchvision import transforms

fashion_mnist_train = datasets.FashionMNIST(root=".",
                                            download=True,
                                            train=True,
                                            transform=transforms.ToTensor())

fashion_mnist_test = datasets.FashionMNIST(root=".",
                                          train=False,
                                          download=True,
                                          transform= transforms.ToTensor())

len(fashion_mnist_train), len(fashion_mnist_test)

# Get the class names of the fashion MNIST dataset
fashion_mnist_class_names = fashion_mnist_train.classes
fashion_mnist_class_names

# Turn Fashion MNIST datasets into dataloaders
from torch.utils.data import dataloader

fashion_mnist_train_dataloader = DataLoader(fashion_mnist_train,
                                            batch_size=32,
                                            shuffle=True)

fashion_mnist_test_dataloader = DataLoader(fashion_mnist_test,
                                           batch_size=32,
                                           shuffle=False)

len(fashion_mnist_train_dataloader), len(fashion_mnist_train_dataloader)

